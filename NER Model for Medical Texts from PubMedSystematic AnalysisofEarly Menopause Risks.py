# -*- coding: utf-8 -*-
"""QUANTIFYING THE VALUE OF DELAYED OVARIAN AGING.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_UkCz3d74xUTZkoTlGP7-vOvam-TVw2K

#COMPONENT 1: AI-POWERED EVIDENCE SYNTHESIS

##NER Model for Medical Texts from PubMed: Systematic Analysis of Early Menopause Risks

This solution implements a Named Entity Recognition (NER) pipeline to extract risk metrics associated with early menopause from PubMed literature.

###1. Solution Architecture

The pipeline consists of 4 key stages:

Data collection from PubMed.

Text preprocessing.

Named Entity Recognition (NER).

Data structuring and credibility assessment.

###2. Dependencies
"""

!pip install biopython
!pip install pubmed-parser pandas numpy scikit-learn nltk spacy

import pandas as pd
import numpy as np
from Bio import Entrez
import spacy
from spacy.training import Example
import re
from collections import defaultdict
import logging
from typing import List, Dict, Tuple

!pip install --upgrade biopython

"""###3. PubMed Data Collection"""

def fetch_pubmed_articles(query: str, max_results: int = 100) -> List[Dict]:
    Entrez.email = "mary.koroleva.2003@gmail.com"  # Required registration
    handle = Entrez.esearch(db="pubmed", term=query, retmax=max_results)
    record = Entrez.read(handle)
    id_list = record["IdList"]

    articles = []
    for uid in id_list:
        handle = Entrez.efetch(db="pubmed", id=uid, retmode="xml")
        article = Entrez.read(handle)
        articles.append(article)
    return articles

"""###Example query:"""

query = "early menopause AND (cardiovascular disease OR osteoporosis OR dementia OR breast cancer)"
articles = fetch_pubmed_articles(query)

"""###4. Text Preprocessing"""

def preprocess_text(text: str) -> str:
    # Remove URLs and special characters
    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)
    text = re.sub(r'[^\w\s\.\,\;\:\-\(\)]', '', text)
    # Normalize whitespace
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

"""###5. NER Model Configuration

Using spaCy with custom patterns for medical terms.


"""

# Load model
nlp = spacy.load("en_core_web_sm")

# Add custom patterns
ruler = nlp.add_pipe("entity_ruler")
patterns = [
    {"label": "HEALTH_CONDITION", "pattern": [{"LOWER": {"IN": ["cardiovascular", "stroke", "myocardial infarction", "osteoporosis", "dementia", "breast cancer"]}}]},
    {"label": "RISK_MEASURE", "pattern": [{"LOWER": "odds"}, {"LOWER": "ratio"}]},
    {"label": "RISK_MEASURE", "pattern": [{"LOWER": "hazard"}, {"LOWER": "ratio"}]},
    {"label": "RISK_MEASURE", "pattern": [{"LOWER": "relative"}, {"LOWER": "risk"}]},
    {"label": "MENOPAUSE_TYPE", "pattern": [{"LOWER": {"IN": ["early", "premature"]}}, {"LOWER": "menopause"}]}
]
ruler.add_patterns(patterns)

"""###6. Entity Extraction"""

import json

# We present the structure of the first article for analysis.
if articles:
    print("Structure of the first article:")
    print(json.dumps(articles[0], indent=2))
else:
    print("No data available for analysis")

def safe_extract_abstract(article: dict) -> str:
    try:
        # 1. Получаем PubmedArticle (это список!)
        pubmed_articles = article.get('PubmedArticle', [])
        if not pubmed_articles:
            return ""

        # 2. Берём первый элемент списка
        pubmed_article = pubmed_articles[0]  # <-- Важно: доступ через [0]!

        # 3. Теперь работаем с MedlineCitation
        medline = pubmed_article.get('MedlineCitation', {})
        article_data = medline.get('Article', {})
        abstract = article_data.get('Abstract', {})
        abstract_text = abstract.get('AbstractText', "")

        # 4. Обрабатываем AbstractText (как раньше)
        if isinstance(abstract_text, list):
            parts = []
            for item in abstract_text:
                if isinstance(item, dict):
                    text_part = item.get('#text', '')
                    label = item.get('Label', '')
                    parts.append(f"{label}: {text_part}" if label else text_part)
                else:
                    parts.append(str(item))
            return " ".join(parts)
        elif isinstance(abstract_text, dict):
            return str(abstract_text.get('#text', ''))
        else:
            return str(abstract_text)

    except (KeyError, TypeError, IndexError, AttributeError) as e:
        print(f"Ошибка при извлечении аннотации: {e}")
        return ""

"""###7. Data Structuring"""

def structure_data(results: List[Dict]) -> pd.DataFrame:
    structured_data = []
    for result in results:
        row = {
            "text": result["text"],
            "health_conditions": ", ".join(result["entities"].get("HEALTH_CONDITION", [])),
            "risk_measures": ", ".join(result["entities"].get("RISK_MEASURE", [])),
            "menopause_type": ", ".join(result["entities"].get("MENOPAUSE_TYPE", []))
        }
        structured_data.append(row)
    return pd.DataFrame(structured_data)

"""###8. Credibility Assessment
Implement a scoring system based on:


*   Source quality (journal impact factor).
*  Sample size (extracted from text).
*   Statistical significance (p-value, CI).

"""

def assess_credibility(text: str) -> float:
    score = 0.0
    # Check for p-value
    if re.search(r'p\s*<\s*0\.{,}05', text):
        score += 0.3
    # Check for sample size
    if re.search(r'n\s*=\s*\d{3,}', text):
        score += 0.2
    # Check for confidence interval
    if re.search(r'95%\s*CI', text):
        score += 0.5
    return min(score, 1.0)

"""###9. Final Pipeline"""

def run_pipeline(query: str, max_results: int = 100) -> pd.DataFrame:
    # 1. Data collection
    articles = fetch_pubmed_articles(query, max_results)

    # 2. Processing and extraction
    results = []
    for article in articles:
        try:
            text = preprocess_text(article["MedlineCitation"]["Article"]["Abstract"]["AbstractText"][0])
            doc = nlp(text)
            entities = extract_entities(doc)
            credibility = assess_credibility(text)
            results.append({
                "text": text,
                "entities": entities,
                "credibility_score": credibility
            })
        except Exception as e:
            logging.error(f"Error processing article: {e}")

    # 3. Structuring
    df = structure_data(results)
    df["credibility_score"] = [r["credibility_score"] for r in results]
    return df

"""###Pipeline Execution"""

df = run_pipeline("early menopause AND cardiovascular disease", max_results=50)
print(df.head())