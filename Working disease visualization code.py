# -*- coding: utf-8 -*-
"""QUANTIFYING THE VALUE OF DELAYED OVARIAN AGING.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_UkCz3d74xUTZkoTlGP7-vOvam-TVw2K

#COMPONENT 1: AI-POWERED EVIDENCE SYNTHESIS

###1. Solution Architecture
"""

# Install required packages (if not already installed)
!pip install pubmed-parser pandas numpy scikit-learn nltk spacy

import pandas as pd
import numpy as np
from pubmed_parser import parse_pubmed_xml
import nltk
from nltk.tokenize import sent_tokenize
import spacy
from sklearn.feature_extraction.text import TfidfVectorizer
import re
import json
from datetime import datetime

"""###2. Define Search Parameters"""

!pip install biopython

!pip install --upgrade biopython

from Bio import Entrez
import time
import os
import pandas as pd
from datetime import datetime

# Target health conditions
HEALTH_CONDITIONS = [
    "cardiovascular disease", "stroke", "myocardial infarction",
    "osteoporosis", "fracture risk",
    "dementia", "Alzheimer's disease",
    "breast cancer",
    "vasomotor symptoms", "cognitive changes", "quality of life"
]

# Key risk measures
RISK_MEASURES = ["OR", "HR", "RR", "odds ratio", "hazard ratio", "relative risk"]

# Criteria for early menopause
EARLY_MENOPAUSE_CRITERIA = [
    "premature ovarian insufficiency", "early menopause", "menopause before 45"
]

# Set your email for NCBI API access
Entrez.email = "your_email@example.com"

# Queries for each health condition
queries = {
    "Cardiovascular_Disease": '(menopause OR menopause transition) AND ("cardiovascular disease"[MeSH Terms] OR heart attack OR stroke)',
    "Osteoporosis_Fractures": '(menopause OR menopause transition) AND ("osteoporosis"[MeSH Terms] OR fracture risk)',
    "Dementia": '(menopause OR menopause transition) AND ("dementia"[MeSH Terms] OR cognitive decline)',
    "Breast_Cancer": '(menopause OR menopause transition) AND ("breast cancer"[MeSH Terms])',
    "Vasomotor_Symptoms": '(menopause OR menopause transition) AND (hot flash OR night sweat)',
    "Cognitive_Changes": '(menopause OR menopause transition) AND ("cognitive function"[MeSH Terms] OR memory change)',
    "Quality_of_Life": '(menopause OR menopause transition) AND ("quality of life"[MeSH Terms] OR well-being)'
}

# Output directory for XML files
output_dir = "pubmed_xml_downloads"
os.makedirs(output_dir, exist_ok=True)

def download_pubmed_xml(query_name: str, query: str, max_results: int = 50):
    print(f"Searching for: {query_name}")
    try:
        handle = Entrez.esearch(db="pubmed", term=query, retmax=max_results)
        record = Entrez.read(handle)
        handle.close()

        id_list = record["IdList"]
        if not id_list:
            print(f"No articles found for query '{query_name}'.")
            return

        print(f"{len(id_list)} articles found. Starting XML download...")
        for i, uid in enumerate(id_list):
            try:
                fetch_handle = Entrez.efetch(db="pubmed", id=uid, retmode="xml")
                xml_data = fetch_handle.read()
                fetch_handle.close()

                filename = f"{query_name}_{uid}.xml"
                filepath = os.path.join(output_dir, filename)
                with open(filepath, "w", encoding="utf-8") as f:
                    f.write(xml_data.decode("utf-8"))


                print(f"[{i+1}/{len(id_list)}] Saved: {filename}")
                time.sleep(0.2)
            except Exception as e:
                print(f"Error downloading article {uid}: {e}")
    except Exception as e:
        print(f"Error searching query {query_name}: {e}")

# Download XML files for each query
for query_name, query in queries.items():
    download_pubmed_xml(query_name, query, max_results=50)
    print("-" * 50)

print("Download completed!")

"""###3. Structuring XML files"""

import os
import xml.etree.ElementTree as ET
import pandas as pd
import json
from datetime import datetime
import re

# Directory with XML files
xml_dir = "pubmed_xml_downloads"
# Output paths
output_csv = "structured_pubmed_data.csv"
output_json = "structured_pubmed_data.json"

import xml.etree.ElementTree as ET
import re

def parse_pubmed_xml(filepath):
    try:
        tree = ET.parse(filepath)
        root = tree.getroot()

        data = {
            "Study_ID": "",
            "Article_Title": "",
            "Authors": [],
            "Publication_Year": "",
            "Source": "",
            "Study_Design_Type": "",
            "Menopause_Onset_Age": "",
            "Health_Condition": "",
            "Primary_Risk_Measure": "",  # OR / HR / RR
            "Measure_Value": "",        # Numeric value (e.g., 2.5)
            "Confidence_Interval": "",   # e.g., 1.8–3.4
            "P_Value": "",               # e.g., p<0.001
            "Sample_Size": "",           # Total N
            "Participant_Characteristics": "",
            "Confounding_Factors_Adjusted": "",
            "Study_Credibility_Assessment": "",
            "Brief_Summary_of_Findings": "",
            # NEW: Detailed risk metrics with context
            "Odds_Ratio": "",           # Extracted OR (if mentioned)
            "Hazard_Ratio": "",         # Extracted HR (if mentioned)
            "Relative_Risk": "",        # Extracted RR (if mentioned)
            "Risk_Comparison_Context": ""  # e.g., "early vs. normal menopause"
        }

        # Study ID
        pmid = root.find(".//PMID")
        if pmid is not None and pmid.text:
            data["Study_ID"] = pmid.text.strip()


        # Article title
        title = root.find(".//ArticleTitle")
        if title is not None and title.text:
            data["Article_Title"] = title.text.strip()
        else:
            data["Article_Title"] = ""

        # Authors
        authors = root.findall(".//Author")
        for author in authors:
            lastname = author.find("LastName")
            forename = author.find("ForeName")
            name = ""
            if lastname is not None and lastname.text:
                name += lastname.text.strip()
            if forename is not None and forename.text:
                if name:
                    name += " "
                name += forename.text.strip()
            if name:
                data["Authors"].append(name)
        data["Authors"] = "; ".join(data["Authors"]) if data["Authors"] else ""


        # Publication year
        pubdate = root.find(".//PubDate/Year")
        if pubdate is not None and pubdate.text:
            data["Publication_Year"] = pubdate.text.strip()
        else:
            data["Publication_Year"] = ""

        # Source (journal)
        journal = root.find(".//Journal/Title")
        if journal is not None and journal.text:
            data["Source"] = journal.text.strip()
        else:
            data["Source"] = ""

        # Abstract processing
        abstract_elem = root.find(".//AbstractText")
        abstract_text = ""
        if abstract_elem is not None:
            if abstract_elem.text and abstract_elem.text.strip():
                abstract_text += abstract_elem.text.strip() + " "
            for p in abstract_elem.findall(".//P"):
                if p.text and p.text.strip():
                    abstract_text += p.text.strip() + " "
            abstract_text = abstract_text.strip().lower()
        else:
            abstract_text = ""

        # Study design type
        if "cohort study" in abstract_text or "cohort" in abstract_text:
            data["Study_Design_Type"] = "Cohort"
        elif "case-control" in abstract_text:
            data["Study_Design_Type"] = "Case-Control"
        elif "randomized controlled trial" in abstract_text:
            data["Study_Design_Type"] = "Randomized Controlled Trial"


        # Menopause onset age
        age_patterns = [
            r"(\d{2})\s*-\s*(\d{2})\s*years",
            r"age\s*(\d{2})",
            r"(\d{2})\s*and\s*older"
        ]
        for pattern in age_patterns:
            match = re.search(pattern, abstract_text)
            if match:
                data["Menopause_Onset_Age"] = match.group(0).strip()
                break

        # Health condition
        health_terms = ["cardiovascular", "osteoporosis", "dementia", "breast cancer", "hot flash"]
        for term in health_terms:
            if term in abstract_text:
                data["Health_Condition"] = term
                break

        # Risk comparison context (NEW)
        context_keywords = [
            "early menopause vs normal",
            "premature menopause",
            "early onset menopause",
            "timely menopause"
        ]
        for keyword in context_keywords:
            if keyword in abstract_text:
                data["Risk_Comparison_Context"] = keyword
                break

        # Extract detailed risk metrics (OR, HR, RR) with context
        risk_metrics = {
            "odds ratio": "Odds_Ratio",
            "hazard ratio": "Hazard_Ratio",
            "relative risk": "Relative_Risk"
        }

        for metric_term, data_key in risk_metrics.items():
            # Search for metric + value + CI + p-value in context
            pattern = rf"{re.escape(metric_term)}\s*[:=]\s*(\d+\.?\d*)\s*(?:\s*\(([\d\.–,]+)\))?\s*(?:\s*p\s*[<>=]\s*([\d\.]+))?"
            match = re.search(pattern, abstract_text, re.IGNORECASE)
            if match:
                data[data_key] = match.group(1)  # Numeric value
                # Extract CI if present
                if match.group(2):
                    data["Confidence_Interval"] = match.group(2).replace(",", "–")
                # Extract p-value if present
                if match.group(3):
                    data["P_Value"] = f"p{match.group(3)}"
                # Set primary measure
                data["Primary_Risk_Measure"] = metric_term.upper()

        # Fallback: if no specific metric found, check general risk phrases
        if not data["Primary_Risk_Measure"]:
            for term in ["odds ratio", "hazard ratio", "relative risk"]:
                if term in abstract_text:
                    data["Primary_Risk_Measure"] = term.upper()
                    break

        # Sample size
        sample_match = re.search(r"n\s*=\s*(\d+)", abstract_text)
        if sample_match:
            data["Sample_Size"] = sample_match.group(1)

        # Brief summary of findings
        if abstract_text:
            sentences = [s.strip() for s in abstract_text.split(". ") if s.strip()]
            conclusion_sentences = [s for s in sentences if "conclusion" in s.lower()]
            if conclusion_sentences:
                data["Brief_Summary_of_Findings"] = conclusion_sentences[0]
            elif len(sentences) >= 2:
                data["Brief_Summary_of_Findings"] = ". ".join(sentences[-2:])
            elif sentences:
                data["Brief_Summary_of_Findings"] = sentences[-1]
        else:
            data["Brief_Summary_of_Findings"] = ""


        return data

    except Exception as e:
        print(f"Error processing {filepath}: {e}")
        return None

all_data = []

for filename in os.listdir(xml_dir):
    if filename.endswith(".xml"):
        filepath = os.path.join(xml_dir, filename)
        parsed_data = parse_pubmed_xml(filepath)
        if parsed_data:
            all_data.append(parsed_data)

print(f"Processed files: {len(all_data)}")

# Save to CSV
df = pd.DataFrame(all_data)
df.to_csv(output_csv, index=False, encoding="utf-8")
print(f"Data saved to {output_csv}")

# Save to JSON
with open(output_json, "w", encoding="utf-8") as f:
    json.dump(all_data, f, ensure_ascii=False, indent=2)
print(f"Data saved to {output_json}")

"""#####(Code - Answer to the solution for SINGULARIS))

###if Menopause_Onset_Age is numeric, convert it into categories:
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# If Menopause_Onset_Age is numeric — convert to categories
if df['Menopause_Onset_Age'].dtype in ['int64', 'float64']:
    bins = [0, 40, 45, 50, 100]
    labels = ['<40', '40–45', '45–50', '>50']
    df['Menopause_Age_Group'] = pd.cut(df['Menopause_Onset_Age'], bins=bins, labels=labels, right=False)
else:
    df['Menopause_Age_Group'] = df['Menopause_Onset_Age']

"""###Bar Chart (Disease Frequency by Age Group)"""

filtered_df = df.dropna(subset=['Health_Condition'])
filtered_df = filtered_df[filtered_df['Health_Condition'].astype(str).str.strip() != '']

plt.figure(figsize=(10, 6))
sns.countplot(data=filtered_df, x='Health_Condition', order=filtered_df['Health_Condition'].value_counts().index)
plt.title('Frequency of Health Conditions (Non-empty values only)')
plt.xlabel('Health Condition')
plt.ylabel('Count')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

"""###Percentage Breakdown by Group"""

valid_cond = df['Health_Condition'].dropna()
valid_cond = valid_cond[valid_cond.astype(str).str.strip() != '']


percentages = valid_cond.value_counts(normalize=True) * 100


plt.figure(figsize=(10, 6))
percentages.plot(kind='bar', color='lightcoral', edgecolor='darkred')
plt.title('Percentage of Health Conditions (Missing Values Ignored)')
plt.xlabel('Health Condition')
plt.ylabel('Percentage (%)')
plt.xticks(rotation=45, ha='right')


for i, v in enumerate(percentages):
    plt.text(i, v + 0.5, f'{v:.1f}%', ha='center', va='bottom', fontsize=9)


plt.ylim(0, max(percentages) + 2)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()